{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Support Vector Machines\n",
    "## Quadratic Programming Problem\n",
    "There are a number of sources that will show that the first step to building a support vector machine is solving the following quadratic programming problem.\n",
    "\n",
    "\\begin{equation}\n",
    "W(\\alpha) = \\sum_i^n \\alpha_i - \\sum_i^n\\sum_j^n \\alpha_i \\alpha_j y_i y_j \\vec{x}_i^T \\vec{x}_j\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\sum_i^n \\alpha_iy_i = 0, \\quad \\quad \\alpha_i \\geq 0\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha_i$ is unknown, $y_i$ is the label, and $x_i$ is the data.\n",
    "\n",
    "These equations have important characteristics to them. The most important are\n",
    "\\begin{equation} \n",
    "\\vec{w} = \\sum_i^n \\alpha_i y_i x_i\n",
    "\\end{equation} where $w$ is a vector that helps to define our decision boundary and that most of the $\\alpha_i$'s will be zero. So, most of the vectors that constitute $w$ will be 0.\n",
    "\n",
    "Therefore, the points closed to our decision boundary will be used to define it.\n",
    "\n",
    "## Kernel Trick\n",
    "Equation (1) works simply in the case of a problem that is linearly seperable. However, in the case of classifying points that are not linearly seperable, we can use something called a kernel trick and modify equation (1) to be\n",
    "\\begin{equation}\n",
    "W(\\alpha) = \\sum_i^n \\alpha_i - \\sum_i^n\\sum_j^n \\alpha_i \\alpha_j y_i y_j K(\\vec{x}_i,\\vec{x}_j)\n",
    "\\end{equation}\n",
    "where $K(\\vec{x}_i, \\vec{x}_j)$ is some transformation of $\\vec{x}_i$ and $\\vec{x}_j$. Some examples of common kernel tricks are\n",
    "\n",
    "1. $(\\vec{x}^T_i \\vec{x}_j)^n$ where $n$ is any power.\n",
    "1. $(\\vec{x}^T_i \\vec{x}_j + b)^n$ where $n$ is any power and $b$ is some bias.\n",
    "1. $e^{||\\vec{x}_i - \\vec{x}_j||^2/\\sigma^2}$ where $\\sigma$ can be modified for various fitting.\n",
    "\n",
    "The only requirement for a kernel trick is that it returns some numerical value that can be considered some kind of distance between points. That is, if $x_i$ and $x_j$ were images, there was some determination of a distance between the two images that comes out of this kernel trick.\n",
    "\n",
    "## Machine Learning Algorithms\n",
    "Understanding these elements, we can begin to write a Support Vector Algorithm for classification. We will start with doing this under the pretense that all data is numerical in nature and so the kernel tricks needed will require no conversion to have numerical returns. \n",
    "\n",
    "The steps to writing a machine learning algorithm are\n",
    "\n",
    "1. Train\n",
    "1. Validate\n",
    "1. Test\n",
    "\n",
    "where using a training sample and equation (2) and (4), we come out with a model. That model is then validated against another sample. Several models will be built with various kernel tricks, and then they will also be validated. The model with the highest accuracy is then chosen and tested against our test set for accuracy and reported on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Packages\n",
    "1. **Numpy**: Numpy is needed for various matrix algebra and added mathematics functions.\n",
    "1. **cvxopt**: cvxopt is used for Quadratic Programming. I don't know how to do it, so following Andrew Tulloch's code, I used it for solving the best weights.\n",
    "1. **matplotlib**: This is explicity used for plotting and data visualization.\n",
    "1. **argh**: Argh allows me to execute examples.\n",
    "1. **itertools**: itertools helps in creating a contour plot. Also adopted from Andrew Tulloch's code.\n",
    "1. **os**: Commly imported package. I use it for quickly reading in every file within a folder.\n",
    "1. **scipy and scimage**: These are used for image transformations. \n",
    "1. **pandas**: Pandas is useful for dataframes. This way, I can label matrices within a single dataset.\n",
    "1. **mnist**: This is used to process images from the mnist database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxopt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import argh\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage.transform import resize\n",
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Functions\n",
    "Before we are able to write a trainer, it is necessary to define a few kernel functions which will be a part of the Kernel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Kernel(object):\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        def f(x, y):\n",
    "            return np.inner(x, y)\n",
    "        return f\n",
    "    \n",
    "    @staticmethod\n",
    "    def polynomial(dim, offset = 0.0):\n",
    "        def f(x, y):\n",
    "            return (np.inner(x,y) + offset) ** dim\n",
    "        return f\n",
    "    \n",
    "    @staticmethod\n",
    "    def radial_basis(sigma):\n",
    "        def f(x, y):\n",
    "            num = la.norm(x - y) ** 2\n",
    "            den = 2*sigma**2\n",
    "            return np.exp(num/den)\n",
    "        return f\n",
    "        \n",
    "    @staticmethod\n",
    "    def gaussian(sigma):\n",
    "        def f(x, y):\n",
    "            num = la.norm(x-y) ** 2\n",
    "            den = (2 * sigma ** 2)\n",
    "            return np.exp(-np.sqrt(num/den))\n",
    "        return f\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(gamma, offset):\n",
    "        def f(x, y):\n",
    "            return np.tanh(gamma * np.dot(x,y) + offset)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Writing a Trainer\n",
    "#### The Predicter Class\n",
    "We also need to have a predicter so that the trainer is able to continue updating itself. Predicting will use the decision boundary equation\n",
    "\\begin{equation}\n",
    "\\sum_i^n \\alpha_i y_i K(\\vec{x_i}, \\vec{x}) + b \\geq 0 \\implies x \\text{ is a positive sample.}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Predicter(object):\n",
    "    def __init__(self, kernel, bias,\n",
    "                alpha, X, labels):\n",
    "        self._kernel = kernel\n",
    "        self._bias = bias\n",
    "        self._alpha = alpha\n",
    "        self._X = X\n",
    "        self._labels = labels\n",
    "    def predict(self, x):\n",
    "        result = self._bias\n",
    "        for a_i, x_i, y_i in zip(self._alpha,\n",
    "                                 self._X,\n",
    "                                 self._labels):\n",
    "            result += a_i*y_i*self._kernel(x_i, x)\n",
    "        return np.sign(result).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This predicter object intializes itself as having the various components needed for labeling an individual point, and then has a function that performs the function. Then, depending on whether the sign is positive or negative, it returns a +1 for positive samples and a -1 for negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Trainer Class\n",
    "Now is the significantly more difficult part. The trainer for SVM utilizes quadratic programming which is honestly something I'm not super familiar with, so I'll be utilizing the cvxopt package to find the proper $\\alpha$'s to maximize $W(\\alpha)$. I will also take a lot of (most of) inspiration from Andrew Tulloch's Trainer in his guide [here](http://tullo.ch/articles/svm-py/).\n",
    "\n",
    "The following is a brief overview of each function within the Trainer class:\n",
    "\n",
    "1. **__init__**: Initializes the trainer with a kernel function and the cost variable used to determine the accuracy of the quadratic programming maximization.\n",
    "1. **train**: Computes the alphas (weights) and creates a predicter class for training iterations.\n",
    "1. **compute_weights**: Computes the weights for the predicter by using quadratic programming to maximize $W(\\alpha)$.\n",
    "1. **create_predicter**: Creates predicter class by converting all below minimal weights to 0 and using all others as weights in predicter. This minimizes computation by only iterating over points with $\\alpha > 0$. I also use Andrew Tulloch's code here to compute the bias which is based on a presentation from Carnegie Mellon.\n",
    "1. **compute_gram**: This creates the Gram Matrix, which in Machine Learning is just a matrix of every $x_i, x_j$ pair in a kernel function where $G_{ij}$ = $K(x_i, x_j)$. The Gram Matrix is necessary for the quadratic programming maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, kernel, cost):\n",
    "        self._kernel = kernel\n",
    "        self._c = cost\n",
    "        \n",
    "    def train(self, X, labels):\n",
    "        weights = self.compute_weights(X, labels)\n",
    "        return self.create_predicter(X, labels, weights)\n",
    "    \n",
    "    def compute_weights(self, X, labels):\n",
    "        n = len(X)\n",
    "        Gram = self.compute_gram(X)\n",
    "        P = cvxopt.matrix(np.outer(labels, labels) * Gram)\n",
    "        q = cvxopt.matrix(-1 * np.ones(n))\n",
    "\n",
    "        # -a_i \\leq 0\n",
    "        # TODO(tulloch) - modify G, h so that we have a soft-margin classifier\n",
    "        G_std = cvxopt.matrix(np.diag(np.ones(n) * -1))\n",
    "        h_std = cvxopt.matrix(np.zeros(n))\n",
    "\n",
    "        # a_i \\leq c\n",
    "        G_slack = cvxopt.matrix(np.diag(np.ones(n)))\n",
    "        h_slack = cvxopt.matrix(np.ones(n) * self._c)\n",
    "\n",
    "        G = cvxopt.matrix(np.vstack((G_std, G_slack)))\n",
    "        h = cvxopt.matrix(np.vstack((h_std, h_slack)))\n",
    "\n",
    "        A = cvxopt.matrix(labels, (1, n))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        # solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, kktsolver='ldl', options={'kktreg':1e-9})\n",
    "        # Lagrange multipliers\n",
    "        return np.ravel(solution['x'])\n",
    "    \n",
    "    def create_predicter(self, X, labels, weights):\n",
    "        \"\"\"non_minimal_indices = weights > 1e-5\n",
    "        X_non_minimal = [X[i] for i in non_minimal_indices if i == True]\n",
    "        labels_non_minimal = [labels[i] for i in non_minimal_indices if i == True]\n",
    "        weights_non_minimal = [weights[i] for i in non_minimal_indices if i == True]\"\"\"\n",
    "\n",
    "        bias = np.mean(\n",
    "            [y_k - Predicter(\n",
    "                kernel=self._kernel,\n",
    "                bias=0.0,\n",
    "                alpha=weights,\n",
    "                X=X,\n",
    "                labels=labels).predict(x_k)\n",
    "            for (y_k, x_k) in zip(labels, X)])\n",
    "        \n",
    "        return Predicter(\n",
    "                kernel = self._kernel,\n",
    "                bias = bias,\n",
    "                alpha = weights,\n",
    "                X = X,\n",
    "                labels = labels)\n",
    "    \n",
    "    \n",
    "    def compute_gram(self, X):\n",
    "        n = len(X)\n",
    "        G = np.zeros((n, n))\n",
    "        for i, x_i in enumerate(X):\n",
    "            for j, x_j in enumerate(X):\n",
    "                G[i, j] = self._kernel(x_i, x_j)\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Tester Class\n",
    "This just takes the predicter class and predicts the label of testing information and returns the predicted labels and whether or not they were correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tester(object):\n",
    "    def __init__ (self, predicter, data, true_labels):\n",
    "        self._predicter = predicter\n",
    "        self._data = data\n",
    "        self._true_labels = true_labels\n",
    "    def compute_accuracy(self):\n",
    "        flatten = lambda m: np.array(m).reshape(-1,)\n",
    "        predictions = [self._predicter.predict(x) for x in self._data]\n",
    "        correct = [flatten(predictions)[i] == flatten(self._true_labels)[i]\n",
    "                   for i in range(len(flatten(predictions)))]\n",
    "        return (predictions, correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Procedure used to cause of a list of lists to be a single list\n",
    "flatten = lambda m: np.array(m).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_example(num_samples=500, num_features=2, grid_size=100, filename = \"svm.pdf\"):\n",
    "    samples = np.matrix(np.random.uniform(low = -1.5, high = 1.5, size=num_samples * num_features)\n",
    "                        .reshape(num_samples, num_features))\n",
    "    \n",
    "    labels = 2 * (samples.sum(axis=1) > 0) - 1.0\n",
    "    flatten = lambda m: np.array(m).reshape(-1,)\n",
    "\n",
    "    \n",
    "    plt.scatter(flatten(samples[:,0]), flatten(samples[:,1]),\n",
    "                c=flatten(labels), cmap=cm.Paired, edgecolor = \"white\",\n",
    "                s = 16)\n",
    "    plt.savefig(\"examples.pdf\")\n",
    "    \n",
    "    training_samples = samples[1:num_samples//2,]\n",
    "    testing_samples = samples[num_samples//2 + 1:,]\n",
    "    training_labels = labels[1:num_samples//2]\n",
    "    testing_labels = labels[num_samples//2 + 1:]\n",
    "    \n",
    "    trainer = Trainer(Kernel.linear(), 0.01)\n",
    "    predicter = trainer.train(training_samples, training_labels)\n",
    "    tester = Tester(predicter, testing_samples, testing_labels)\n",
    "    predicted_labels, correct = tester.compute_accuracy()\n",
    "    \n",
    "    plot(predicter, training_samples, training_labels, testing_samples, \n",
    "         predicted_labels, correct, grid_size, \"linearcase.pdf\")\n",
    "    \n",
    "def polynomial_example(num_samples=500, num_features=2, grid_size=100, filename = \"svm.pdf\"):\n",
    "    samples = np.matrix(np.random.uniform(low = -1.5, high = 1.5, size=num_samples * num_features)\n",
    "                        .reshape(num_samples, num_features))\n",
    "    labels = 2 * (np.sqrt(np.power(samples[:,0],2) + np.power(samples[:,1],2)) > 1) - 1.0\n",
    "    \n",
    "    training_samples = samples[1:num_samples//2,]\n",
    "    testing_samples = samples[num_samples//2 + 1:,]\n",
    "    training_labels = labels[1:num_samples//2]\n",
    "    testing_labels = labels[num_samples//2 + 1:]\n",
    "    \n",
    "    plt.scatter(flatten(samples[:,0]), flatten(samples[:,1]),\n",
    "                c=flatten(labels), cmap=cm.Paired, edgecolor = \"white\",\n",
    "                s = 16)\n",
    "    plt.savefig(\"poly.pdf\")\n",
    "    \n",
    "    \n",
    "    trainer = Trainer(Kernel.polynomial(2), 0.01)\n",
    "    predicter = trainer.train(samples, labels)\n",
    "    tester = Tester(predicter, testing_samples, testing_labels)\n",
    "    predicted_labels, correct = tester.compute_accuracy()\n",
    "    \n",
    "    plot(predicter, training_samples, training_labels, testing_samples, \n",
    "         predicted_labels, correct, grid_size, \"polynomialcase.pdf\")\n",
    "\n",
    "\n",
    "def plot(predicter, training_X, training_y, testing_X, testing_y,\n",
    "         correct, grid_size, filename):\n",
    "    \n",
    "    flatten = lambda m: np.array(m).reshape(-1,)\n",
    "    x_min, x_max = training_X[:, 0].min() - 0.25, training_X[:, 0].max() + .25\n",
    "    y_min, y_max = training_X[:, 1].min() - 0.25, training_X[:, 1].max() + .25\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, grid_size),\n",
    "                         np.linspace(y_min, y_max, grid_size),\n",
    "                         indexing='ij')\n",
    "    flatten = lambda m: np.array(m).reshape(-1,)\n",
    "    result = []\n",
    "    for (i, j) in itertools.product(range(grid_size), range(grid_size)):\n",
    "        point = np.array([xx[i, j], yy[i, j]]).reshape(1, 2)\n",
    "        result.append(predicter.predict(point))\n",
    "    Z = np.array(result).reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z,\n",
    "                 cmap=cm.Paired,\n",
    "                 levels=[-0.001, 0.001],\n",
    "                 extend='both',\n",
    "                 alpha=0.8)\n",
    "    plt.scatter(flatten(training_X[:, 0]), flatten(training_X[:, 1]),\n",
    "                c=flatten(training_y), cmap=cm.Paired, edgecolor = \"white\",\n",
    "                s = 16)\n",
    "    colors = [\"white\" if x == True else \"red\" for x in correct]\n",
    "    plt.scatter(flatten(testing_X[:, 0]), flatten(testing_X[:, 1]),\n",
    "                c = flatten(testing_y), cmap = cm.Paired, edgecolor = colors,\n",
    "                s =16)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.savefig(filename)\n",
    "    plt.clf()\n",
    "\n",
    "    \n",
    "argh.dispatch_command(linear_example)\n",
    "argh.dispatch_command(polynomial_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons to LDA, QDA, and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def linear_comparison(num_samples=1000, num_features=2, grid_size=100, filename = \"svm.pdf\"):\n",
    "    samples = np.array(np.random.uniform(low = -1.5, high = 1.5, size=num_samples * num_features)\n",
    "                        .reshape(num_samples, num_features))\n",
    "    \n",
    "    labels = 2 * (samples.sum(axis=1) > 0) - 1.0\n",
    "    \n",
    "    train_samples = samples[1:len(samples)//2,]\n",
    "    train_labels = labels[1:len(samples)//2]\n",
    "    test_samples = samples[(len(samples)//2 + 1):,]\n",
    "    test_labels = labels[(len(samples)//2 + 1):,]\n",
    "    \n",
    "    \n",
    "    trainer = Trainer(Kernel.linear(), 0.01)\n",
    "    predicter = trainer.train(train_samples, train_labels)\n",
    "    tester = Tester(predicter, test_samples, test_labels)\n",
    "    predicted_labels, correct = tester.compute_accuracy()\n",
    "    svm_accuracy = sum(correct)/len(correct)\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(train_samples, train_labels)\n",
    "    correct = (lda.predict(test_samples) == test_labels)\n",
    "    lda_accuracy = sum(correct)/len(correct)\n",
    "    \n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    qda.fit(train_samples, train_labels)\n",
    "    correct = (qda.predict(test_samples) == test_labels)\n",
    "    qda_accuracy = sum(correct)/len(correct)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(train_samples, train_labels)\n",
    "    correct = (knn.predict(test_samples) == test_labels)\n",
    "    knn_accuracy = sum(correct)/len(correct)\n",
    "    \n",
    "    return(svm_accuracy, lda_accuracy, qda_accuracy, knn_accuracy)\n",
    "\n",
    "def polynomial_comparison(num_samples=1000, num_features=2, grid_size=100, filename = \"svm.pdf\"):\n",
    "    samples = np.array(np.random.uniform(low = -1.5, high = 1.5, size=num_samples * num_features)\n",
    "                        .reshape(num_samples, num_features))\n",
    "    labels = 2 * (np.sqrt(np.power(samples[:,0],2) + np.power(samples[:,1],2)) > 1) - 1.0\n",
    "    \n",
    "    train_samples = samples[1:len(samples)//2,]\n",
    "    train_labels = labels[1:len(samples)//2]\n",
    "    test_samples = samples[(len(samples)//2 + 1):,]\n",
    "    test_labels = labels[(len(samples)//2 + 1):,]\n",
    "    \n",
    "    trainer = Trainer(Kernel.polynomial(2), 0.01)\n",
    "    predicter = trainer.train(train_samples, train_labels)\n",
    "    tester = Tester(predicter, test_samples, test_labels)\n",
    "    predicted_labels, correct = tester.compute_accuracy()\n",
    "    svm_accuracy = sum(correct)/len(correct)\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(train_samples, train_labels)\n",
    "    correct = (lda.predict(test_samples) == test_labels)\n",
    "    lda_accuracy = sum(correct)/len(correct)\n",
    "    \n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    qda.fit(train_samples, train_labels)\n",
    "    correct = (qda.predict(test_samples) == test_labels)\n",
    "    qda_accuracy = sum(correct)/len(correct)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(train_samples, train_labels)\n",
    "    correct = (knn.predict(test_samples) == test_labels)\n",
    "    knn_accuracy = sum(correct)/len(correct)\n",
    "    \n",
    "    return(svm_accuracy, lda_accuracy, qda_accuracy, knn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_tests = np.mat([linear_comparison() for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polynomial_tests = np.mat([polynomial_comparison() for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Linear SVM-Average:\", np.mean(linear_tests[0:,]))\n",
    "print(\"Linear LDA-Average:\", np.mean(linear_tests[1:,]))\n",
    "print(\"Linear QDA-Average:\", np.mean(linear_tests[2:,]))\n",
    "print(\"Linear KNN-Average:\", np.mean(linear_tests[3:,]))\n",
    "print(\"Quadratic SVM-Average:\", np.mean(polynomial_tests[0:,]))\n",
    "print(\"Quadratic LDA-Average:\", np.mean(polynomial_tests[1:,]))\n",
    "print(\"Quadratic QDA-Average:\", np.mean(polynomial_tests[2:,]))\n",
    "print(\"Quadratic KNN-Average:\", np.mean(polynomial_tests[3:,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "This returns the odd format of the .idx3 files as a numpy matrix so that is processes properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(num):\n",
    "    mat = np.mat(num)\n",
    "    return(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Reading\n",
    "Using the MNIST package (which was created to read this database), the information is processed and sorted into a training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('samples')\n",
    "\n",
    "train_images, train_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "train_nums = [preprocessing(x) for x in train_images]\n",
    "test_nums = [preprocessing(x) for x in test_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Subsets\n",
    "Because I haven't written a routine to handle classification of multiple classes, the below code creates a subset of samples that are only labeled 1 or 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Dataframes of Training and Testing Images\n",
    "train_df = pd.DataFrame()\n",
    "train_df[\"Numbers\"] = train_nums\n",
    "train_df[\"Labels\"] = train_labels\n",
    "train_ones = train_df[train_df[\"Labels\"] == 1]\n",
    "train_fives = train_df[train_df[\"Labels\"] == 5]\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df[\"Numbers\"] = test_nums\n",
    "test_df[\"Labels\"] = test_labels\n",
    "test_ones = test_df[test_df[\"Labels\"] == 1]\n",
    "test_fives = test_df[test_df[\"Labels\"] == 5]\n",
    "\n",
    "train_samples = list(train_ones[\"Numbers\"]) + list(train_fives[\"Numbers\"])\n",
    "train_labels = list(train_ones[\"Labels\"]) + list(train_fives[\"Labels\"])\n",
    "test_samples = list(test_ones[\"Numbers\"]) + list(test_fives[\"Numbers\"])\n",
    "test_labels = list(test_ones[\"Labels\"]) + list(test_fives[\"Labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training/Testing\n",
    "This just uses similar methods to above examples to show accuracy of the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(Kernel.polynomial(2), .1)\n",
    "predicter = trainer.train(train_samples[0:1000], train_labels[0:1000])\n",
    "tester = Tester(predicter, test_samples[0:1000], test_labels[0:1000])\n",
    "predicted_labels, correct = tester.compute_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute Accuracy\n",
    "sum(correct)/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
