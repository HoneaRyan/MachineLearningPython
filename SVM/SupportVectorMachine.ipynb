{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "  MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "  });"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "  %%javascript\n",
    "    MathJax.Hub.Config({\n",
    "      TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "    });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Support Vector Machines\n",
    "## Quadratic Programming Problem\n",
    "There are a number of sources that will show that the first step to building a support vector machine is solving the following quadratic programming problem.\n",
    "\n",
    "\\begin{equation}\n",
    "W(\\alpha) = \\sum_i^n \\alpha_i - \\sum_i^n\\sum_j^n \\alpha_i \\alpha_j y_i y_j \\vec{x}_i^T \\vec{x}_j\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\sum_i^n \\alpha_iy_i = 0, \\quad \\quad \\alpha_i \\geq 0\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha_i$ is unknown, $y_i$ is the label, and $x_i$ is the data.\n",
    "\n",
    "These equations have important characteristics to them. The most important are\n",
    "\\begin{equation} \n",
    "\\vec{w} = \\sum_i^n \\alpha_i y_i x_i\n",
    "\\end{equation} where $w$ is a vector that helps to define our decision boundary and that most of the $\\alpha_i$'s will be zero. So, most of the vectors that constitute $w$ will be 0.\n",
    "\n",
    "Therefore, the points closed to our decision boundary will be used to define it.\n",
    "\n",
    "## Kernel Trick\n",
    "Equation (1) works simply in the case of a problem that is linearly seperable. However, in the case of classifying points that are not linearly seperable, we can use something called a kernel trick and modify equation (1) to be\n",
    "\\begin{equation}\n",
    "W(\\alpha) = \\sum_i^n \\alpha_i - \\sum_i^n\\sum_j^n \\alpha_i \\alpha_j y_i y_j K(\\vec{x}_i,\\vec{x}_j)\n",
    "\\end{equation}\n",
    "where $K(\\vec{x}_i, \\vec{x}_j)$ is some transformation of $\\vec{x}_i$ and $\\vec{x}_j$. Some examples of common kernel tricks are\n",
    "\n",
    "1. $(\\vec{x}^T_i \\vec{x}_j)^n$ where $n$ is any power.\n",
    "1. $(\\vec{x}^T_i \\vec{x}_j + b)^n$ where $n$ is any power and $b$ is some bias.\n",
    "1. $e^{||\\vec{x}_i - \\vec{x}_j||^2/\\sigma^2}$ where $\\sigma$ can be modified for various fitting.\n",
    "\n",
    "The only requirement for a kernel trick is that it returns some numerical value that can be considered some kind of distance between points. That is, if $x_i$ and $x_j$ were images, there was some determination of a distance between the two images that comes out of this kernel trick.\n",
    "\n",
    "## Algorithm Writing\n",
    "Understanding these elements, we can begin to write a Support Vector Algorithm for classification. We will start with doing this under the pretense that all data is numerical in nature and so the kernel tricks needed will require no conversion to have numerical returns. \n",
    "\n",
    "The steps to writing a machine learning algorithm are\n",
    "\n",
    "1. Train\n",
    "1. Validate\n",
    "1. Test\n",
    "\n",
    "where using a training sample and equation (2) and (4), we come out with a model. That model is then validated against another sample. Several models will be built with various kernel tricks, and then they will also be validated. The model with the highest accuracy is then chosen and tested against our test set for accuracy and reported on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Packages\n",
    "Numpy: Numpy is needed for various matrix algebra and added mathematics functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxopt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import argh\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Functions\n",
    "Before we are able to write a trainer, it is necessary to define a few kernel functions which will be a part of the Kernel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Kernel(object):\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        def f(x, y):\n",
    "            return np.inner(x, y)\n",
    "        return f\n",
    "    \n",
    "    @staticmethod\n",
    "    def polynomial(dim, offset):\n",
    "        def f(x, y):\n",
    "            return (np.inner(x,y) + offset) ** dim\n",
    "        return f\n",
    "    \n",
    "    @staticmethod\n",
    "    def radial_basis(sigma):\n",
    "        def f(x, y):\n",
    "            num = la.norm(x - y) ** 2\n",
    "            den = 2*sigma**2\n",
    "            return np.exp(num/den)\n",
    "        return f\n",
    "        \n",
    "    @staticmethod\n",
    "    def gaussian(sigma):\n",
    "        def f(x, y):\n",
    "            num = la.norm(x-y) ** 2\n",
    "            den = (2 * sigma ** 2)\n",
    "            return np.exp(-np.sqrt(num/den))\n",
    "        return f\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(gamma, offset):\n",
    "        def f(x, y):\n",
    "            return np.tanh(gamma * np.dot(x,y) + offset)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Writing a Predicter\n",
    "We also need to have a predicter so that the trainer is able to continue updating itself. Predicting will use the decision boundary equation\n",
    "\\begin{equation}\n",
    "\\sum_i^n \\alpha_i y_i K(\\vec{x_i}, \\vec{x}) + b \\geq 0 \\implies x \\text{ is a positive sample.}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Predicter(object):\n",
    "    def __init__(self, kernel, bias,\n",
    "                alpha, X, labels):\n",
    "        self._kernel = kernel\n",
    "        self._bias = bias\n",
    "        self._alpha = alpha\n",
    "        self._X = X\n",
    "        self._labels = labels\n",
    "    def predict(self, x):\n",
    "        result = self._bias\n",
    "        for a_i, x_i, y_i in zip(self._alpha,\n",
    "                                 self._X,\n",
    "                                 self._labels):\n",
    "            result += a_i*y_i*self._kernel(x_i, x)\n",
    "        return np.sign(result).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This predicter object intializes itself as having the various components needed for labeling an individual point, and then has a function that performs the function. Then, depending on whether the sign is positive or negative, it returns a +1 for positive samples and a -1 for negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a Trainer\n",
    "Now is the significantly more difficult part. The trainer for SVM utilizes quadratic programming which is honestly something I'm not super familiar with, so I'll be utilizing the cvxopt package to find the proper $\\alpha$'s to maximize $W(\\alpha)$. I will also take a lot of (most of) inspiration from Andrew Tulloch's Trainer in his guide [here](http://tullo.ch/articles/svm-py/).\n",
    "\n",
    "The following is a brief overview of each function within the Trainer class:\n",
    "\n",
    "1. **__init__**: Initializes the trainer with a kernel function and the cost variable used to determine the accuracy of the quadratic programming maximization.\n",
    "1. **train**: Computes the alphas (weights) and creates a predicter class for training iterations.\n",
    "1. **compute_weights**: Computes the weights for the predicter by using quadratic programming to maximize $W(\\alpha)$.\n",
    "1. **create_predicter**: Creates predicter class by converting all below minimal weights to 0 and using all others as weights in predicter. This minimizes computation by only iterating over points with $\\alpha > 0$. I also use Andrew Tulloch's code here to compute the bias which is based on a presentation from Carnegie Mellon.\n",
    "1. **compute_gram**: This creates the Gram Matrix, which in Machine Learning is just a matrix of every $x_i, x_j$ pair in a kernel function where $G_{ij}$ = $K(x_i, x_j)$. The Gram Matrix is necessary for the quadratic programming maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, kernel, cost):\n",
    "        self._kernel = kernel\n",
    "        self._c = cost\n",
    "        \n",
    "    def train(self, X, labels):\n",
    "        weights = self.compute_weights(X, labels)\n",
    "        return self.create_predicter(X, labels, weights)\n",
    "    \n",
    "    def compute_weights(self, X, labels):\n",
    "        n, m = X.shape\n",
    "        Gram = self.compute_gram(X)\n",
    "        P = cvxopt.matrix(np.outer(labels, labels) * Gram)\n",
    "        q = cvxopt.matrix(-1 * np.ones(n))\n",
    "\n",
    "        # -a_i \\leq 0\n",
    "        # TODO(tulloch) - modify G, h so that we have a soft-margin classifier\n",
    "        G_std = cvxopt.matrix(np.diag(np.ones(n) * -1))\n",
    "        h_std = cvxopt.matrix(np.zeros(n))\n",
    "\n",
    "        # a_i \\leq c\n",
    "        G_slack = cvxopt.matrix(np.diag(np.ones(n)))\n",
    "        h_slack = cvxopt.matrix(np.ones(n) * self._c)\n",
    "\n",
    "        G = cvxopt.matrix(np.vstack((G_std, G_slack)))\n",
    "        h = cvxopt.matrix(np.vstack((h_std, h_slack)))\n",
    "\n",
    "        A = cvxopt.matrix(labels, (1, n))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        return np.ravel(solution['x'])\n",
    "    \n",
    "    def create_predicter(self, X, labels, weights):\n",
    "        non_minimal_indices = weights > 1e-5\n",
    "        X_non_minimal = X[non_minimal_indices]\n",
    "        labels_non_minimal = labels[non_minimal_indices]\n",
    "        weights_non_minimal = weights[non_minimal_indices]\n",
    "\n",
    "        bias = np.mean(\n",
    "            [y_k - Predicter(\n",
    "                kernel=self._kernel,\n",
    "                bias=0.0,\n",
    "                alpha=weights_non_minimal,\n",
    "                X=X_non_minimal,\n",
    "                labels=labels_non_minimal).predict(x_k)\n",
    "            for (y_k, x_k) in zip(labels_non_minimal, X_non_minimal)])\n",
    "        \n",
    "        return Predicter(\n",
    "                kernel = self._kernel,\n",
    "                bias = bias,\n",
    "                alpha = weights_non_minimal,\n",
    "                X = X_non_minimal,\n",
    "                labels = labels_non_minimal)\n",
    "    \n",
    "    \n",
    "    def compute_gram(self, X):\n",
    "        n, m = X.shape\n",
    "        G = np.zeros((n, n))\n",
    "        for i, x_i in enumerate(X):\n",
    "            for j, x_j in enumerate(X):\n",
    "                G[i, j] = self._kernel(x_i, x_j)\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.0145e+01 -1.0157e+01  3e+03  5e+01  5e-15\n",
      " 1: -2.9069e+00 -1.0104e+01  4e+01  6e-01  5e-15\n",
      " 2: -2.0133e+00 -6.8514e+00  5e+00  2e-16  1e-15\n",
      " 3: -2.3218e+00 -2.9188e+00  6e-01  4e-17  9e-16\n",
      " 4: -2.5902e+00 -2.6831e+00  9e-02  5e-17  8e-16\n",
      " 5: -2.6241e+00 -2.6541e+00  3e-02  7e-17  8e-16\n",
      " 6: -2.6321e+00 -2.6470e+00  1e-02  4e-17  7e-16\n",
      " 7: -2.6362e+00 -2.6433e+00  7e-03  1e-16  7e-16\n",
      " 8: -2.6389e+00 -2.6408e+00  2e-03  2e-16  9e-16\n",
      " 9: -2.6397e+00 -2.6401e+00  4e-04  3e-16  9e-16\n",
      "10: -2.6399e+00 -2.6399e+00  4e-06  2e-16  9e-16\n",
      "11: -2.6399e+00 -2.6399e+00  4e-08  5e-17  9e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "def example(num_samples=500, num_features=2, grid_size=50, filename=\"svm.pdf\"):\n",
    "    samples = np.matrix(np.random.uniform(low = -1.5, high = 1.5, size=num_samples * num_features)\n",
    "                        .reshape(num_samples, num_features))\n",
    "    labels = 2 * (np.sqrt(np.power(samples[:,0],2) + np.power(samples[:,1],2)) > 1) - 1.0\n",
    "    trainer = Trainer(Kernel.polynomial(2, 0), 0.01)\n",
    "    predictor = trainer.train(samples, labels)\n",
    "\n",
    "    plot(predictor, samples, labels, grid_size, filename)\n",
    "\n",
    "\n",
    "def plot(predictor, X, y, grid_size, filename):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, grid_size),\n",
    "                         np.linspace(y_min, y_max, grid_size),\n",
    "                         indexing='ij')\n",
    "    flatten = lambda m: np.array(m).reshape(-1,)\n",
    "\n",
    "    result = []\n",
    "    for (i, j) in itertools.product(range(grid_size), range(grid_size)):\n",
    "        point = np.array([xx[i, j], yy[i, j]]).reshape(1, 2)\n",
    "        result.append(predictor.predict(point))\n",
    "\n",
    "    Z = np.array(result).reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z,\n",
    "                 cmap=cm.Paired,\n",
    "                 levels=[-0.001, 0.001],\n",
    "                 extend='both',\n",
    "                 alpha=0.8)\n",
    "    plt.scatter(flatten(X[:, 0]), flatten(X[:, 1]),\n",
    "                c=flatten(y), cmap=cm.Paired, edgecolor = \"white\",\n",
    "                s = 9)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.savefig(\"stuff.pdf\")\n",
    "\n",
    "    \n",
    "argh.dispatch_command(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
